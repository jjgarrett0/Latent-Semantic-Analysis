{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis - Lab\n",
    " \n",
    "I'll be mining concepts from the 'rec.sport.baseball' collections of newsgroups. I will perform TF-IDF Vectorizing and Latent Semantic Analysis on the corpus as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #Beautiful Soup is a Python library for pulling data out of HTML and XML files. \n",
    "import nltk #Natural Language Toolkit.\n",
    "from nltk.corpus import stopwords #used for high-frequency words like the, to and also that we sometimes want to filter out of a document before further processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #Used to extract features in a format supported by machine learning algorithms.\n",
    "                                                            #TfidfVectorizer - Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "from sklearn.decomposition import TruncatedSVD #Dimensionality reduction using truncated SVD (aka LSA).\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/JG/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this only once\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the rec.sport.baseball dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['rec.sport.baseball']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(0,len(corpus)):\n",
    "    # Use regular expressions to do a find-and-replace\n",
    "    corpus[idx] = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                         \" \",                   # The pattern to replace it with\n",
    "                         corpus[idx])  # The text to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'From  writingctr leo bsuvc bsu edu Subject  Re  CUB fever  Organization  Ball State University  Muncie  In   Univ  Computing Svc s Lines       In article  kingoz           camelot   kingoz camelot bradley edu  Orin Roth  writes          CUB fever is hitting me again  I m beginning to think they have a       chance this year   what the heck am i thinking        Sorry  Just a moment of incompetence       I ll be ok  Really        Orin       Bradley U            I m really a jester in disguise                                     I hear ya   Then again  we must remember that we are indeed Cub fans  and that the Cubs will eventually blow it   After all  the Cubs are the easiest team in the National League to root for   No Pressure   You know they will lose eventually   Oh well  I suppose we must have faith   After all  they do look pretty good  and they don t even have Sandberg back yet     CUBS IN           CHA '"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'From  hamkins geisel csl uiuc edu  Jon Hamkins  Subject  Re  Triva question on Bosio s No hitter Organization  Center for Reliable and High Performance Computing  University of Illinois at Urbana Champaign Lines     NNTP Posting Host  grinch csl uiuc edu  wall cc swarthmore edu  Matthew Wall  writes    I don t actually have the answer to this one    Bosio  after walking the first two batters  retired    straight for a   back end  perfect game   Well  there were    outs in a row with no hits or walks in between  but really  he only retired    batters in a row   The first out of the game was the front end of a double play   Still counts as a back end perfect game in my book  though    Congrats to Chris Bosio   Too bad the Brewers couldn t hold on to him            Jon Hamkins   hamkins uiuc edu           University of Illionois '"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stopwords using the standard \"English\" Stop word directory. Also add custom stopwords to get a better understanding of valuable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['\\n','com','edu','cs','nntp','cs','Re:','@','>','--','vb30',\n",
    "                'lafibm','posting','host','ibm','subject','reply','would',\n",
    "                'come','university','ca','adobe','said',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing\n",
    "\n",
    "I'm going to use scikit-learn's TF-IDF vectorizer to take my corpus and convert each document into a sparse matrix of TFIDF Features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'From  writingctr leo bsuvc bsu edu Subject  Re  CUB fever  Organization  Ball State University  Muncie  In   Univ  Computing Svc s Lines       In article  kingoz           camelot   kingoz camelot bradley edu  Orin Roth  writes          CUB fever is hitting me again  I m beginning to think they have a       chance this year   what the heck am i thinking        Sorry  Just a moment of incompetence       I ll be ok  Really        Orin       Bradley U            I m really a jester in disguise                                     I hear ya   Then again  we must remember that we are indeed Cub fans  and that the Cubs will eventually blow it   After all  the Cubs are the easiest team in the National League to root for   No Pressure   You know they will lose eventually   Oh well  I suppose we must have faith   After all  they do look pretty good  and they don t even have Sandberg back yet     CUBS IN           CHA '"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before!\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset,\n",
    "                                 use_idf=True, ngram_range=(1, 2)) #This will convert a collection of raw documents\n",
    "                                                                   #into matrix of TF-IDF features.\n",
    "X = vectorizer.fit_transform(corpus) #Learn vocabulary and idf, return term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x82613 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 137 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15871)\t0.0965237950924\n",
      "  (0, 82246)\t0.0965237950924\n",
      "  (0, 5083)\t0.0965237950924\n",
      "  (0, 61669)\t0.0910953309428\n",
      "  (0, 21650)\t0.0965237950924\n",
      "  (0, 28321)\t0.0910953309428\n",
      "  (0, 55381)\t0.0602677692641\n",
      "  (0, 40643)\t0.0965237950924\n",
      "  (0, 22845)\t0.0965237950924\n",
      "  (0, 46151)\t0.0965237950924\n",
      "  (0, 69938)\t0.0965237950924\n",
      "  (0, 78888)\t0.0965237950924\n",
      "  (0, 48823)\t0.0763868460347\n",
      "  (0, 21717)\t0.0965237950924\n",
      "  (0, 40866)\t0.0965237950924\n",
      "  (0, 36969)\t0.0965237950924\n",
      "  (0, 55343)\t0.0965237950924\n",
      "  (0, 60450)\t0.0965237950924\n",
      "  (0, 38163)\t0.0965237950924\n",
      "  (0, 46461)\t0.0589917337094\n",
      "  (0, 71153)\t0.0965237950924\n",
      "  (0, 19919)\t0.0965237950924\n",
      "  (0, 15876)\t0.0965237950924\n",
      "  (0, 8340)\t0.0965237950924\n",
      "  (0, 21710)\t0.0965237950924\n",
      "  :\t:\n",
      "  (0, 12000)\t0.0514416306135\n",
      "  (0, 72181)\t0.0295297613638\n",
      "  (0, 6902)\t0.0704714831393\n",
      "  (0, 31589)\t0.0449770952442\n",
      "  (0, 80701)\t0.0204306655198\n",
      "  (0, 60573)\t0.0725352894259\n",
      "  (0, 50087)\t0.145070578852\n",
      "  (0, 9226)\t0.130086037979\n",
      "  (0, 10660)\t0.137367465634\n",
      "  (0, 36636)\t0.145070578852\n",
      "  (0, 3677)\t0.0211317806745\n",
      "  (0, 39528)\t0.013469217432\n",
      "  (0, 70249)\t0.0797515038978\n",
      "  (0, 14171)\t0.0509877485057\n",
      "  (0, 76121)\t0.0514416306135\n",
      "  (0, 46042)\t0.0797515038978\n",
      "  (0, 68021)\t0.0489239422192\n",
      "  (0, 5337)\t0.0402303614823\n",
      "  (0, 49788)\t0.0134963191416\n",
      "  (0, 23641)\t0.163630620369\n",
      "  (0, 15826)\t0.211414449418\n",
      "  (0, 9856)\t0.0663829603457\n",
      "  (0, 9866)\t0.0663829603457\n",
      "  (0, 38651)\t0.0650430189897\n",
      "  (0, 81076)\t0.0965237950924\n"
     ]
    }
   ],
   "source": [
    "#After\n",
    "print X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA\n",
    "\n",
    "Input:  X, a matrix where m is the number of documents I have, and n is the number of terms.\n",
    "\n",
    "Process:   I'm going to decompose X into three matricies called U, S, and T.  When we do the decomposition, we have to pick a value k, that's how many concepts we are going to keep.  \n",
    "\n",
    "$$X \\approx USV^{T}$$\n",
    "\n",
    "U will be a m x k matrix.  The rows will be documents and the columns will be 'concepts'\n",
    "\n",
    "S will be a k x k diagnal matrix.   The elements will be the amount of variation captured from each concept.\n",
    "\n",
    "V will be a n x k (mind the transpose) matrix.   The rows will be terms and the columns will be conepts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994, 82613)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=10, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=10, n_iter=100)\n",
    "lsa.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0078717 ,  0.00131177,  0.00059651, ...,  0.00206443,\n",
       "        0.00058628,  0.00058628])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the first row for V\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.12 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:43:17) \n",
      "[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "year\n",
      "team\n",
      "game\n",
      "writes\n",
      "article\n",
      "baseball\n",
      "games\n",
      "players\n",
      "one\n",
      "good\n",
      " \n",
      "Concept 1:\n",
      "morris\n",
      "lost\n",
      "think\n",
      "sox\n",
      "see\n",
      "season\n",
      "many\n",
      "apr\n",
      "year\n",
      "one\n",
      " \n",
      "Concept 2:\n",
      "games\n",
      "braves\n",
      "season\n",
      "league\n",
      "organization\n",
      "baseball\n",
      "better\n",
      "two\n",
      "way\n",
      "lost\n",
      " \n",
      "Concept 3:\n",
      "year\n",
      "one\n",
      "braves\n",
      "season\n",
      "well\n",
      "say\n",
      "article\n",
      "news\n",
      "jewish\n",
      "last year\n",
      " \n",
      "Concept 4:\n",
      "year\n",
      "team\n",
      "braves\n",
      "play\n",
      "well\n",
      "know\n",
      "game\n",
      "apr\n",
      "john\n",
      "hit\n",
      " \n",
      "Concept 5:\n",
      "hitter\n",
      "years\n",
      "article apr\n",
      "let\n",
      "hit\n",
      "john\n",
      "well\n",
      "know\n",
      "time\n",
      "gant\n",
      " \n",
      "Concept 6:\n",
      "well\n",
      "better\n",
      "year\n",
      "last\n",
      "sox\n",
      "two\n",
      "make\n",
      "win\n",
      "many\n",
      "players\n",
      " \n",
      "Concept 7:\n",
      "organization\n",
      "team\n",
      "new\n",
      "aa atlanta\n",
      "article\n",
      "lines\n",
      "like\n",
      "cubs\n",
      "sox\n",
      "think\n",
      " \n",
      "Concept 8:\n",
      "year\n",
      "baseball\n",
      "aa freenet\n",
      "aa formerly\n",
      "lines\n",
      "much\n",
      "aa improve\n",
      "back\n",
      "article apr\n",
      "season\n",
      " \n",
      "Concept 9:\n",
      "writes\n",
      "time\n",
      "games\n",
      "year\n",
      "good\n",
      "aa formerly\n",
      "article\n",
      "baseball\n",
      "aa improve\n",
      "aa atlanta\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print \"Concept %d:\" % i\n",
    "    for term in sortedTerms:\n",
    "        print term[0]\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0078717 ,  0.00131177,  0.00059651, ...,  0.00206443,\n",
       "         0.00058628,  0.00058628],\n",
       "       [ 0.0012602 , -0.01029942,  0.01007711, ..., -0.00201067,\n",
       "        -0.00015524, -0.00015524],\n",
       "       [-0.01217207, -0.01625772,  0.04494135, ...,  0.00031474,\n",
       "        -0.00108783, -0.00108783],\n",
       "       ..., \n",
       "       [-0.02564566,  0.00427531,  0.06703105, ..., -0.0021611 ,\n",
       "         0.000294  ,  0.000294  ],\n",
       "       [ 0.00202877, -0.01223918, -0.09818833, ...,  0.0027089 ,\n",
       "         0.00030748,  0.00030748],\n",
       "       [ 0.03800456,  0.00392628, -0.03595597, ...,  0.0008463 ,\n",
       "         0.00122033,  0.00122033]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [IPython]",
   "language": "python",
   "name": "Python [IPython]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
